<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
	<title>集群级分片分配和路由设置 | ElasticSearch 7.7 权威指南中文版</title>
	<meta name="keywords" content="ElasticSearch 权威指南中文版, elasticsearch 7, es7, 实时数据分析，实时数据检索" />
    <meta name="description" content="ElasticSearch 权威指南中文版, elasticsearch 7, es7, 实时数据分析，实时数据检索" />
    <!-- Give IE8 a fighting chance -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
	<link rel="stylesheet" type="text/css" href="../static/styles.css" />
	<script>
	var _link = 'modules-cluster.html';
    </script>
</head>
<body>
<div class="main-container">
    <section id="content">
        <div class="content-wrapper">
            <section id="guide" lang="zh_cn">
                <div class="container">
                    <div class="row">
                        <div class="col-xs-12 col-sm-8 col-md-8 guide-section">
                            <div style="color:gray; word-break: break-all; font-size:12px;">原英文版地址: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/modules-cluster.html" rel="nofollow" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/7.7/modules-cluster.html</a>, 原文档版权归 www.elastic.co 所有<br/>本地英文版地址: <a href="../en/modules-cluster.html" rel="nofollow" target="_blank">../en/modules-cluster.html</a></div>
                        <!-- start body -->
                  <div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch 权威指南 [7.7]</a></span>
»
<span class="breadcrumb-link"><a href="setup.html">安装和设置</a></span>
»
<span class="breadcrumb-link"><a href="settings.html">配置 Elasticsearch</a></span>
»
<span class="breadcrumb-node">集群级分片分配和路由设置</span>
</div>
<div class="navheader">
<span class="prev">
<a href="circuit-breaker.html">« 熔断机制设置</a>
</span>
<span class="next">
<a href="ccr-settings.html">跨集群复制设置 »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title">
<a id="modules-cluster"></a>
集群级分片分配和路由设置 (Cluster-level shard allocation and routing settings)
</h2>
</div></div></div>
<p>
<em>分片分配 (shard allocation)</em> 是分配分片到节点的过程。

这可能在 初始化恢复(initial recovery)、副本分配(replica allocation)、再平衡(rebalancing)，或 添加/删除节点时发生。
</p>
<p>
主节点的主要角色之一是决定将哪些分片分配到哪个节点，以及何时在节点之间移动分片，以再平衡集群。
</p>
<p>
有许多可用于控制分片分配过程的设置：
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-settings" title="Cluster-level shard allocation settings">集群级(cluster-level)分片分配设置</a> 控制分配和再平衡操作。
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#disk-based-shard-allocation" title="Disk-based shard allocation settings">基于磁盘(disk-based)的分片分配设置</a> 解释 Elasticsearch 如何根据可用磁盘空间来分配分片以及相关设置。
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#shard-allocation-awareness" title="Shard allocation awareness">分片分配感知 (shard allocation awareness)</a> 和 <a class="xref" href="modules-cluster.html#forced-awareness" title="Forced awareness">强制感知 (forced awareness)</a> 控制如何在不同的机架或可用区域之间分发分片。
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">集群级(cluster-level)分片分配 过滤 (filtering)</a> 允许某些节点或一组节点被排除在分配之外，以便它们可以被关闭。
</li>
</ul>
</div>
<p>
除此之外，还有其他一些<a class="xref" href="modules-cluster.html#misc-cluster-settings" title="Miscellaneous cluster settings">其他的集群级设置</a>。
</p>
<p>
这些设置都是<em>动态的 (dynamic)</em>，而且可以使用 <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API 在运行中的集群上更新。
</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="cluster-shard-allocation-settings"></a>
集群级(cluster-level)分片分配设置
</h3>
</div></div></div>
<p>
以下 <em>动态的 (dynamic)</em> 设置可以用来控制分片分配和恢复：
</p>
<div class="variablelist">
<a id="cluster-routing-allocation-enable"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.enable</code>
</span>
</dt>
<dd>
<p>
启用或禁用对特定类型的分片的分配：
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">all</code> -             (默认) 允许为所有类型的分片分配分片。
</li>
<li class="listitem">
<code class="literal">primaries</code> -       仅允许为主分片分配分片。
</li>
<li class="listitem">
<code class="literal">new_primaries</code> -   仅允许为新索引的主分片分配分片。
</li>
<li class="listitem">
<code class="literal">none</code> -            任何索引都不允许任何类型的分片。
</li>
</ul>
</div>
<p>
这个设置不影响重启一个节点时本地主分片的恢复。

具有未分配主分片副本的节点重启后，如果其分配 id 与群集状态下的活动的分配 id 之一匹配，则将立即恢复该主分片。
</p>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_incoming_recoveries</code>
</span>
</dt>
<dd>
一个节点上允许并发的 传入分片(incoming shard) 恢复的个数。

传入恢复(incoming recovery) 是在节点上分配目标分片(很可能是副本，除非分片被重新定位)的恢复。默认为 <code class="literal">2</code>。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_outgoing_recoveries</code>
</span>
</dt>
<dd>
一个节点上允许并发的 传出分片(outgoing shard) 恢复的个数。  

传出恢复(outgoing recovery)是在节点上分配源分片(很可能是主分片，除非分片被重新定位)的恢复。默认为 <code class="literal">2</code>。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_recoveries</code>
</span>
</dt>
<dd>
同时设置<code class="literal">cluster.routing.allocation.node_concurrent_incoming_recoveries</code> 和 <code class="literal">cluster.routing.allocation.node_concurrent_outgoing_recoveries</code>的快捷方式。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_initial_primaries_recoveries</code>
</span>
</dt>
<dd>
虽然副本的恢复发生在网络上，但是节点重启后恢复未分配的主节点时使用的是本地磁盘中的数据。

这些恢复应该很快，这样就可以在同一个节点上并行地进行更多的初始主分片恢复。默认为<code class="literal">4</code>。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.same_shard.host</code>
</span>
</dt>
<dd>
允许根据主机名和主机地址执行检查，以防止在单个主机上分配同一分片的多个实例。

默认为 <code class="literal">false</code>，即不执行检查。

这个设置仅适用于在同一台机器上启动了多个节点。
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="shards-rebalancing-settings"></a>
分片再平衡设置
</h3>
</div></div></div>
<p>
以下 <em>动态的 (dynamic)</em> 设置可以用于控制跨集群的分片的再平衡：
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.rebalance.enable</code>
</span>
</dt>
<dd>
<p>
启用或禁用对特定类型的分片的再平衡：
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">all</code> -         (默认)  允许各种分片的分片再平衡
</li>
<li class="listitem">
<code class="literal">primaries</code> -   仅允许主分片的分片再平衡
</li>
<li class="listitem">
<code class="literal">replicas</code> -    仅允许副本分片的分片再平衡
</li>
<li class="listitem">
<code class="literal">none</code> -        任何索引都不允许任何类型的分片平衡。
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.allow_rebalance</code>
</span>
</dt>
<dd>
<p>
指定什么时候允许分片再平衡：
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">always</code> -                    始终允许再平衡
</li>
<li class="listitem">
<code class="literal">indices_primaries_active</code> -  仅当集群中的所有主分片已分配(处于激活状态)
</li>
<li class="listitem">
<code class="literal">indices_all_active</code> -        (默认) 仅当集群中的所有分片(主和副本)已分配(处于激活状态)
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.cluster_concurrent_rebalance</code>
</span>
</dt>
<dd>
允许控制集群范围内允许多少个并发的分片再平衡。默认为 <code class="literal">2</code>。

请注意，此设置仅控制由于集群不平衡而导致的并发分片重定位的数量。

此设置不会限制由于 <a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">分配过滤 (allocation filtering)</a> 或 <a class="xref" href="modules-cluster.html#forced-awareness" title="Forced awareness">强制感知 (forced awareness)</a> 而导致的分片重新定位。
</dd>
</dl>
</div>
<div>
更多阅读参考: <a href="https://zhuanlan.zhihu.com/p/164970344" rel="nofollow">ElasticSearch集群shard均衡策略</a> (v6.x, 来自知乎)
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="shards-rebalancing-heuristics"></a>
分片平衡启发式设置 (shard balancing heuristics settings)
</h3>
</div></div></div>
<p>
下面的设置一起用于确定每个分片的放置位置。

当任何允许的再平衡操作都不能使任何节点的权重(weight) 与 任何其他节点的权重 之间的密切性比 <code class="literal">balance.threshold</code> 更接近时，集群就是平衡的<span style="color:#666; font-size:80%;">(原文: The cluster is balanced when no allowed rebalancing operation can bring the weight of any node closer to the weight of any other node by more than the balance.threshold.)</span>。
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.shard</code>
</span>
</dt>
<dd>
定义节点上分配的分片总数的权重(weight)因子(浮点型)。默认为 <code class="literal">0.45f</code>。

提高这个值会增加集群中所有节点之间分片数量均衡的趋势。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.index</code>
</span>
</dt>
<dd>
定义在特定节点上分配的每个索引的分片数的权重因子(浮点型)。默认为 <code class="literal">0.55f</code>。

提高这个值会使集群中所有节点的每个索引的分片数量更加趋于均衡。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.threshold</code>
</span>
</dt>
<dd>
应该执行的最小优化值(非负浮点数)。默认为 <code class="literal">1.0f</code>。

提高该值将导致集群在优化分片平衡方面不那么积极。
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
不管平衡算法的结果如何，强制感知(forced awareness) 或 分配过滤(allocation filtering) 可能会导致不允许重新平衡。
</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="disk-based-shard-allocation"></a>
基于磁盘的分片分配设置 (Disk-based shard allocation settings)
</h3>
</div></div></div>
<p>
Elasticsearch 会考量节点上的可用磁盘空间，然后决定是向该节点分配新的分片，还是主动将分片从该节点移走。
</p>
<p>
以下是可以在配置文件 <code class="literal">elasticsearch.yml</code> 中配置的设置，或者使用 <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API 在实时集群上动态更新的设置：
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.threshold_enabled</code>
</span>
</dt>
<dd>
默认为 <code class="literal">true</code>。设置为 <code class="literal">false</code> 将禁用磁盘分配决策器。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.low</code>
</span>
</dt>
<dd>
控制磁盘使用的 低 水位线(watermark)。默认为 <code class="literal">85%</code>，表示 Elasticsearch 不会给磁盘使用率超过 85% 的节点分配分片。

也可以将其设置为绝对字节值(如<code class="literal">500mb</code>)，以防止Elasticsearch 在可用空间少于指定的值时分配分片。

该设置对新创建的索引的主分片没有影响，但会阻止它们的副本的分配。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.high</code>
</span>
</dt>
<dd>
控制磁盘使用的 高 水位线(watermark)。默认为 <code class="literal">90%</code>，表示 Elasticsearch 将尝试从磁盘使用率超过 90% 的节点上移走分片。

也可以将其设置为绝对字节值(类似于低水位线)，以便在节点的可用空间少于指定的值时将分片从该节点移走并重新定位。

这个设置影响所有分片的分配，无论之前是否已分配。
</dd>
</dl>
</div>
<div class="variablelist">
<a id="cluster-routing-flood_stage"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.flood_stage</code>
</span>
</dt>
<dd>
<p>
控制洪水位水位线，默认为 95%。

Elasticsearch 在每个索引上强制执行一个只读索引块(<code class="literal">index.blocks.read_only_allow_delete</code>)，该索引块在节点上分配了一个或多个分片，并且至少有一个磁盘超过了洪水位。

此设置是防止节点磁盘空间耗尽的最后手段。

当磁盘使用率低于高水位线时，索引块会自动释放。
</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
在这些设置中，不能混合使用百分比值和字节值。

要么所有值都设置为百分比值，要么所有值都设置为字节值。

这样做是为了让 Elasticsearch 可以验证设置在内部是一致的，确保低磁盘阈值小于高磁盘阈值，高磁盘阈值小于洪水位阈值。
</p>
</div>
</div>
<p>
重置索引<code class="literal">twitter</code>上的只读索引块的示例：
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /twitter/_settings
{
  "index.blocks.read_only_allow_delete": null
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/19.console"></div>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.info.update.interval</code>
</span>
</dt>
<dd>
Elasticsearch 应该多久检查一次集群中每个节点的磁盘使用情况。默认 <code class="literal">30s</code>。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.include_relocations</code>
</span>
</dt>
<dd>
<span class="Admonishment Admonishment--change">
[<span class="Admonishment-version u-mono u-strikethrough">7.5.0</span>]
<span class="Admonishment-detail">
在 7.5.0 版本中废弃。未来的版本将始终考虑 重定位(relocations)。  
</span>
</span>
默认为 <code class="literal">true</code>，这意味着在计算节点的磁盘使用量时，Elasticsearch 会将当前重定位到目标节点的分片考虑在内。

然而，考虑重定位分片的大小，可能意味着节点的磁盘使用量被错误地估计为偏高，因为重定位可能完成了90%，而最近检索的磁盘使用量会包括重定位分片的总大小以及正在运行的重定位已经使用的空间。
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
百分比值是指已用磁盘空间，而字节值是指可用磁盘空间。

这可能会令人困惑，因为它颠倒了高和低的含义。

例如，将低水位设置为 10gb，高水位设置为 5gb 是有意义的，但反过来则不行。
</p>
</div>
</div>
<p>
下面的例子, 将低水位线更新为至少 100gb 字节可用空间，高水位线更新为至少 50gb 字节可用空间，洪水位水位线更新为 10gb 字节可用空间，并每分钟更新一次集群的信息:
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.disk.watermark.low": "100gb",
    "cluster.routing.allocation.disk.watermark.high": "50gb",
    "cluster.routing.allocation.disk.watermark.flood_stage": "10gb",
    "cluster.info.update.interval": "1m"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/20.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="shard-allocation-awareness"></a>
分片分配感知 (shard allocation awareness)
</h3>
</div></div></div>
<p>
您可以使用自定义节点属性作为<em>感知属性(awareness attributes)</em>，使Elasticsearch 在分配分片时考虑物理硬件配置。

如果 Elasticsearch 知道哪些节点在同一个物理服务器上、在同一个机架上，或者在同一个区域中，它可以分发主分片及其副本分片，以最大限度地降低故障时丢失所有分片副本的风险。
</p>
<p>
当使用<code class="literal">cluster.routing.allocation.awareness.attributes</code>设置启用分片分配感知时，分片仅分配给具有为指定感知属性设置的值的节点。

如果使用了多个感知属性，Elasticsearch 在分配分片时会单独考虑每个属性。
</p>
<p>
分配感知设置可以在<code class="literal">elasticsearch.yml</code>中配置，也可以使用<a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API动态更新。
</p>
<p>
默认情况下，Elasticsearch 使用 <a class="xref" href="search.html#search-adaptive-replica" title="Adaptive Replica Selection">自适应副本选择(adaptive replica selection)</a> 来路由搜索或 GET 请求。

然而，由于分配感知属性的存在，Elasticsearch 将更倾向于使用相同位置的分片(具有相同的感知属性值)来处理这些请求。

要禁用此行为，可以在集群的每个节点上指定系统属性<code class="literal">export ES_JAVA_OPTS="$ES_JAVA_OPTS -Des.search.ignore_awareness_attributes=true"</code>。
</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
属性值的数量决定了在每个位置分配多少个分片的副本。

如果每个位置的节点数量不均衡，且有大量副本，则副本分片可能会保持未分配状态。
</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="enabling-awareness"></a>
启用分片分配感知
</h4>
</div></div></div>
<p>
要启用分片分配感知:
</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>
使用自定义节点属性指定每个节点的位置。

例如，如果希望 Elasticsearch 将分片分布在不同的机架上，可以在每个节点的配置文件 <code class="literal">elasticsearch.yml</code> 中设置一个名为<code class="literal">rack_id</code> <span class="remark">(机架id)</span>的感知属性。
</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">node.attr.rack_id: rack_one</pre>
</div>
<p>
还可以在启动节点时设置自定义属性:
</p>
<div class="pre_wrapper lang-sh">
<pre class="programlisting prettyprint lang-sh">`./bin/elasticsearch -Enode.attr.rack_id=rack_one`</pre>
</div>
</li>
<li class="listitem">
<p>
通过在<span class="strong strong"><strong>每个</strong></span>符合主节点条件的节点的配置文件 <code class="literal">elasticsearch.yml</code> 中设置<code class="literal">cluster.routing.allocation.awareness.attributes</code>，告诉 Elasticsearch 在分配分片时考虑一个或多个感知属性。
</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">cluster.routing.allocation.awareness.attributes: rack_id <a id="CO7-1"></a><i class="conum" data-value="1"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO7-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>
以逗号分隔的列表形式指定多个属性。
</p>
</td>
</tr>
</table>
</div>
<p>
还可以使用 <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API来设置或更新集群的感知属性。
</p>
</li>
</ol>
</div>
<p>
在此示例配置中，如果启动两个节点且将 <code class="literal">node.attr.rack_id</code> 设置为 <code class="literal">rack_one</code> <span class="remark">(机架1)</span>，并创建一个包含 5 个主分片和每个主分片 1 个副本的索引，则所有主分片和副本都将在这两个节点之间分配。
</p>
<p>
如果在 <code class="literal">node.attr.rack_id</code> 设置为 <code class="literal">rack_two</code><span class="remark">(机架2)</span> 的情况下添加两个节点，Elasticsearch 会将分片移动到新节点，确保(如果可能的话)没有同一个分片的两个副本都在同一个机架中。
</p>
<p>
如果 <code class="literal">rack_two</code><span class="remark">(机架2)</span> 出现故障并关闭了它的两个节点，默认情况下，Elasticsearch 会将丢失的分片副本分配给 <code class="literal">rack_one</code> <span class="remark">(机架1)</span> 上的节点。

为了防止在同一位置分配特定分片的多个副本，可以启用 <em>强制感知 (forced awareness)</em>。
</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="forced-awareness"></a>
强制感知 (forced awareness)
</h4>
</div></div></div>
<p>
默认情况下，如果一个位置失效，Elasticsearch 会将所有丢失的副本分片分配给其余位置。

虽然可能在所有的位置上都有足够的资源来承载主分片和副本分片，但是单个位置可能无法承载<span class="strong strong"><strong>所有的</strong></span>分片。
</p>
<p>
为了防止单个位置在发生故障时超负荷，可以设置 <code class="literal">cluster.routing.allocation.awareness.force</code>，以便在其他位置的节点可用之前不会分配副本。
</p>
<p>
例如，如果有一个名为<code class="literal">zone</code>的感知属性，并在<code class="literal">zone1</code>和<code class="literal">zone2</code>中配置节点，可以使用强制感知来防止 Elasticearch 在只有一个区域可用时分配副本：
</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">cluster.routing.allocation.awareness.attributes: zone
cluster.routing.allocation.awareness.force.zone.values: zone1,zone2 <a id="CO8-1"></a><i class="conum" data-value="1"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO8-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>为感知属性指定所有可能的值。</p>
</td>
</tr>
</table>
</div>
<p>
使用上面这个示例的配置，如果启动两个节点，并将<code class="literal">node.attr.zone</code> 设置为 <code class="literal">zone1</code>，并创建一个包含 5 个分片和 1 个副本的索引，则 Elasticsearch 将创建索引并分配 5 个主分片，但不分配副本。副本只有在 <code class="literal">node.attr.zone</code> 值为<code class="literal">zone2</code> 的节点可用时才会分配。
</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="cluster-shard-allocation-filtering"></a>
集群级分片分配过滤 (cluster-level shard allocation filtering)
</h3>
</div></div></div>
<p>
可以使用群集级别的分片分配过滤器来控制 Elasticearch 从任何索引中分配分片的位置。

这些集群范围的过滤器与 <a class="xref" href="shard-allocation-filtering.html" title="Index-level shard allocation filtering">每个索引的分配过滤</a> 和 <a class="xref" href="modules-cluster.html#shard-allocation-awareness" title="Shard allocation awareness">分配感知</a> 一起应用。
</p>
<p>
分片分配过滤可以基于自定义的节点属性，或者内置的<code class="literal">_name</code>, <code class="literal">_host_ip</code>, <code class="literal">_publish_ip</code>, <code class="literal">_ip</code>, <code class="literal">_host</code> 和 <code class="literal">_id</code>属性。
</p>
<p>
<code class="literal">cluster.routing.allocation</code> 是动态的，允许将活动索引从一组节点移动到另一组节点。

只有在不破坏另一个路由约束的情况下，才能重新定位分片，例如永远不要在同一节点上分配主分片和副本分片。
</p>
<p>
集群级分片分配过滤最常见的使用场景是当你想 关闭<span class="remark">(decommission)</span> 一个节点时。

若要在关闭节点之前将分片移出节点，可以创建一个过滤器，根据节点的 IP 地址将其排除在外：
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "transient" : {
    "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/21.console"></div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-routing-settings"></a>
集群路由设置
</h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.include.{attribute}</code>
</span>
</dt>
<dd>
将分片分配给 <code class="literal">{attribute}</code> 至少有一个逗号分隔值的节点。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.require.{attribute}</code>
</span>
</dt>
<dd>
仅将分片分配给 <code class="literal">{attribute}</code> 具有<em>所有</em>逗号分隔值的节点。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.exclude.{attribute}</code>
</span>
</dt>
<dd>
不要将分片分配给其<code class="literal">{attribute}</code>具有<em>任何</em>逗号分隔值的节点。
</dd>
</dl>
</div>
<p>
集群分配设置支持以下几个内置的属性：
</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col>
<col>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">_name</code>
</p>
</td>
<td valign="top">
<p>
按节点名匹配节点
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_host_ip</code>
</p>
</td>
<td valign="top">
<p>
通过主机IP地址匹配节点(IP与主机名关联)
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_publish_ip</code>
</p>
</td>
<td valign="top">
<p>
通过外部IP地址匹配节点
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_ip</code>
</p>
</td>
<td valign="top">
<p>
匹配 <code class="literal">_host_ip</code> 或 <code class="literal">_publish_ip</code>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_host</code>
</p>
</td>
<td valign="top">
<p>
按主机名匹配节点
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_id</code>
</p>
</td>
<td valign="top">
<p>
按节点id匹配节点
</p>
</td>
</tr>
</tbody>
</table>
</div>
<p>指定属性的值时可以使用通配符，例如：</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.exclude._ip": "192.168.2.*"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/22.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="misc-cluster-settings"></a>
其他集群设置
</h3>
</div></div></div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-read-only"></a>元数据 (metadata)
</h4>
</div></div></div>
<p>可以使用以下<em>动态(dynamic)</em>设置将整个集群设置为只读：</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.blocks.read_only</code>
</span>
</dt>
<dd>
使整个集群只读(索引不接受写操作)，不允许修改元数据(创建或删除索引)。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.blocks.read_only_allow_delete</code>
</span>
</dt>
<dd>
与 <code class="literal">cluster.blocks.read_only</code>相同，但允许删除索引以释放资源。
</dd>
</dl>
</div>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>
不要依赖此设置来阻止对群集的更改。
任何有权访问 <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API 的用户都可以让集群再次可读写。
</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-shard-limit"></a>
集群分片限制
</h4>
</div></div></div>
<p>
基于集群中的节点数量，集群中的分片数量有一个软限制。这是为了防止可能无意中破坏集群稳定性的操作。
</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>
该限制旨在作为一种保护措施，而非大小建议。
集群可以安全支持的分片的确切数量取决于硬件配置和工作负载，但几乎所有情况下都应保持在该限制以下，因为默认限制设置得相当高。
</p>
</div>
</div>
<p>
如果某项操作（如创建新索引、恢复索引的快照或打开关闭的索引）会导致群集中的分片数量超过此限制，则该操作将失败，并给出错误，指示分片限制。
</p>
<p>
如果集群由于节点成员的变化或设置的变化已经超过限制，则所有创建或打开索引的操作都将失败，直到增加限制(如下所述),或者<a class="xref" href="indices-open-close.html" title="Open index API">关闭</a>或<a class="xref" href="indices-delete-index.html" title="Delete index API">删除</a>一些索引以使分片数低于这个限制。
</p>
<p>
副本数量计入此限制，但关闭的索引不计入。
包含 5 个主分片和 2 个副本的索引将被计为 15 个分片。
任何关闭的索引都计为 0，不管它包含多少分片和副本。
</p>
<p>
该限制默认为每个数据节点 1,000 个分片，可以使用以下属性进行动态调整：
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.max_shards_per_node</code>
</span>
</dt>
<dd>
控制群集中每个数据节点允许的分片数量。
</dd>
</dl>
</div>
<p>
例如，使用默认设置的 3 个节点的集群将允许所有打开的索引中共有 3,000 个分片。

如果将上述设置更改为 500，那么集群将总共允许 1,500 个分片。
</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
如果群集中没有数据节点，将不会强制实施该限制。如果在数据节点之前设置专用的主节点，将允许在集群创建期间创建索引。
</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="user-defined-data"></a>
用户定义的集群元数据 (user-defined cluster metadata)
</h4>
</div></div></div>
<p>
可以使用 集群设置(Cluster Settings) API 存储和检索用户定义的元数据。
这可以用来存储关于集群的任意、不经常改变的数据，而不需要创建索引来存储这些数据。
可以使用以 <code class="literal">cluster.metadata.</code>为前缀的任何键来存储该数据。
例如，要在键 <code class="literal">cluster.metadata.administrator</code>下存储集群管理员的 email 地址，请发送以下请求:
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /_cluster/settings
{
  "persistent": {
    "cluster.metadata.administrator": "sysadmin@example.com"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/23.console"></div>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>
用户定义的集群元数据不要用于存储敏感或机密信息。
存储在用户定义的集群元数据中的任何信息都可以被任何能够访问 <a class="xref" href="cluster-get-settings.html" title="Cluster get settings API">集群获取设置</a> API 的人查看，并被记录在 Elasticsearch 日志中。
</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-max-tombstones"></a>
索引墓碑 (index tombstones)
</h4>
</div></div></div>
<p>
集群状态维护 <em>索引墓碑 (index tombstones)</em>，以明确表示已被删除的索引。
在群集状态下维护的 墓碑 数量由下面这个不能动态更新的属性控制:
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.indices.tombstones.size</code>
</span>
</dt>
<dd>
索引墓碑可防止发生删除时不属于群集的节点加入群集并重新导入索引，就好像从未执行过删除一样 <span class="remark">(原文: Index tombstones prevent nodes that are not part of the cluster when a delete occurs from joining the cluster and reimporting the index as though the delete was never issued.)</span>。

为了防止集群状态变得过大，只保留最后的<code class="literal">cluster.indices.tombstones.size</code> 个删除，默认为 500。

如果你期望节点不在集群中，并且错过的删除次数超过500次，则可以增加该值<span class="remark">(原文: You can increase it if you expect nodes to be absent from the cluster and miss more than 500 deletes.)</span>。

我们认为这是罕见的，因此使用这个默认值。

墓碑不占太多空间，但我们也觉得 50,000 这样的数字可能太大了。
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-logger"></a>
日志 (logger)
</h4>
</div></div></div>
<p>
控制记录的设置可以用 <code class="literal">logger.</code> 前缀动态更新。

例如，要提高 <code class="literal">indices.recovery</code> 模块的日志记录级别到 <code class="literal">DEBUG</code>，请发送以下请求:
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /_cluster/settings
{
  "transient": {
    "logger.org.elasticsearch.indices.recovery": "DEBUG"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/24.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="persistent-tasks-allocation"></a>
持久任务分配 (persistent tasks allocation)
</h4>
</div></div></div>
<p>
插件可以创建一种叫做<em>持久任务 (persistent tasks)</em>的任务。
这些任务通常是长期存在的任务，存储在集群状态中，允许在整个集群重新启动后重新启动这些任务。  
</p>
<p>
每次创建持久任务时，主节点负责将任务分配给集群中的一个节点，然后被分配的节点将接受该任务并在本地执行。
将持久任务分配给节点的过程由下面几个可以动态更新的属性控制：
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.persistent_tasks.allocation.enable</code>
</span>
</dt>
<dd>
<p>启用或禁用持久任务的分配：</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">all</code> -             (默认) 允许给节点分配持久任务
</li>
<li class="listitem">
<code class="literal">none</code> -            任何类型的持久任务都不允许分配
</li>
</ul>
</div>
<p>
此设置不影响已经在执行的持久任务。
只有新创建的持久任务或必须重新分配的任务(例如，节点离开群集后)才受此设置的影响。
</p>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.persistent_tasks.allocation.recheck_interval</code>
</span>
</dt>
<dd>
当集群状态发生重大变化时，主节点将自动检查是否需要分配持久任务。

但是，可能有其他因素，如内存使用率，会影响持久任务是否可以分配给节点，但又不会导致群集状态发生变化。

此设置控制为响应这些因素而执行分配检查的频率。

默认为 30 秒。允许的最小值为 10 秒。
</dd>
</dl>
</div>
</div>

</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="circuit-breaker.html">« 熔断机制设置</a>
</span>
<span class="next">
<a href="ccr-settings.html">跨集群复制设置 »</a>
</span>
</div>
</div>

                  <!-- end body -->
                        </div>
                        <div class="col-xs-12 col-sm-4 col-md-4" id="right_col">
                        
                        </div>
                    </div>
                </div>
            </section>
        </div>
    </section>
</div>
<script src="../static/cn.js"></script>
</body>
</html>