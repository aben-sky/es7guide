<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="UTF-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
	<title>集群级分片分配和路由设置 | ElasticSearch 7.7 权威指南中文版</title>
	<meta name="keywords" content="ElasticSearch 权威指南中文版, elasticsearch 7, es7, 实时数据分析，实时数据检索" />
    <meta name="description" content="ElasticSearch 权威指南中文版, elasticsearch 7, es7, 实时数据分析，实时数据检索" />
    <!-- Give IE8 a fighting chance -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
	<link rel="stylesheet" type="text/css" href="../static/styles.css" />
	<script>
	var _link = 'modules-cluster.html';
    </script>
</head>
<body>
<div class="main-container">
    <section id="content">
        <div class="content-wrapper">
            <section id="guide" lang="zh_cn">
                <div class="container">
                    <div class="row">
                        <div class="col-xs-12 col-sm-8 col-md-8 guide-section">
                            <div style="color:gray; word-break: break-all; font-size:12px;">原英文版地址: <a href="https://www.elastic.co/guide/en/elasticsearch/reference/7.7/modules-cluster.html" rel="nofollow" target="_blank">https://www.elastic.co/guide/en/elasticsearch/reference/7.7/modules-cluster.html</a>, 原文档版权归 www.elastic.co 所有<br/>本地英文版地址: <a href="../en/modules-cluster.html" rel="nofollow" target="_blank">../en/modules-cluster.html</a></div>
                        <!-- start body -->
                  <div class="page_header">
<strong>IMPORTANT</strong>: No additional bug fixes or documentation updates
will be released for this version. For the latest information, see the
<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html">current release documentation</a>.
</div>
<div id="content">
<div class="breadcrumbs">
<span class="breadcrumb-link"><a href="index.html">Elasticsearch 权威指南 [7.7]</a></span>
»
<span class="breadcrumb-link"><a href="setup.html">安装和设置</a></span>
»
<span class="breadcrumb-link"><a href="settings.html">配置 Elasticsearch</a></span>
»
<span class="breadcrumb-node">集群级分片分配和路由设置</span>
</div>
<div class="navheader">
<span class="prev">
<a href="circuit-breaker.html">« 熔断机制设置</a>
</span>
<span class="next">
<a href2="ccr-settings.html">Cross-cluster replication settings »</a>
</span>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h2 class="title">
<a id="modules-cluster"></a>
集群级分片分配和路由设置 (Cluster-level shard allocation and routing settings)
</h2>
</div></div></div>
<p>
<em>分片分配 (shard allocation)</em> 是分配分片到节点的过程。

这可能在 初始化恢复(initial recovery)、副本分配(replica allocation)、再平衡(rebalancing)，或 添加/删除节点时发生。
</p>
<p>
主节点的主要角色之一是决定将哪些分片分配到哪个节点，以及何时在节点之间移动分片，以再平衡集群。
</p>
<p>
有许多可用于控制分片分配过程的设置：
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-settings" title="Cluster-level shard allocation settings">集群级(cluster-level)分片分配设置</a> 控制分配和再平衡操作。
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#disk-based-shard-allocation" title="Disk-based shard allocation settings">基于磁盘(disk-based)的分片分配设置</a> 解释 Elasticsearch 如何根据可用磁盘空间来分配分片以及相关设置。
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#shard-allocation-awareness" title="Shard allocation awareness">分片分配感知 (shard allocation awareness)</a> 和 <a class="xref" href="modules-cluster.html#forced-awareness" title="Forced awareness">强制感知 (forced awareness)</a> 控制如何在不同的机架或可用区域之间分发分片。
</li>
<li class="listitem">
<a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">集群级(cluster-level)分片分配 过滤 (filtering)</a> 允许某些节点或一组节点被排除在分配之外，以便它们可以被关闭。
</li>
</ul>
</div>
<p>
除此之外，还有其他一些<a class="xref" href="modules-cluster.html#misc-cluster-settings" title="Miscellaneous cluster settings">其他的集群级设置</a>。
</p>
<p>
这些设置都是<em>动态的 (dynamic)</em>，而且可以使用 <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API 在运行中的集群上更新。
</p>
<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="cluster-shard-allocation-settings"></a>
集群级(cluster-level)分片分配设置
</h3>
</div></div></div>
<p>
以下 <em>动态的 (dynamic)</em> 设置可以用来控制分片分配和恢复：
</p>
<div class="variablelist">
<a id="cluster-routing-allocation-enable"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.enable</code>
</span>
</dt>
<dd>
<p>
启用或禁用对特定类型的分片的分配：
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">all</code> -             (默认) 允许为所有类型的分片分配分片。
</li>
<li class="listitem">
<code class="literal">primaries</code> -       仅允许为主分片分配分片。
</li>
<li class="listitem">
<code class="literal">new_primaries</code> -   仅允许为新索引的主分片分配分片。
</li>
<li class="listitem">
<code class="literal">none</code> -            任何索引都不允许任何类型的分片。
</li>
</ul>
</div>
<p>
这个设置不影响重启一个节点时本地主分片的恢复。

具有未分配主分片副本的节点重启后，如果其分配 id 与群集状态下的活动的分配 id 之一匹配，则将立即恢复该主分片。
</p>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_incoming_recoveries</code>
</span>
</dt>
<dd>
一个节点上允许并发的 传入分片(incoming shard) 恢复的个数。

传入恢复(incoming recovery) 是在节点上分配目标分片(很可能是副本，除非分片被重新定位)的恢复。默认为 <code class="literal">2</code>。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_outgoing_recoveries</code>
</span>
</dt>
<dd>
一个节点上允许并发的 传出分片(outgoing shard) 恢复的个数。  

传出恢复(outgoing recovery)是在节点上分配源分片(很可能是主分片，除非分片被重新定位)的恢复。默认为 <code class="literal">2</code>。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_concurrent_recoveries</code>
</span>
</dt>
<dd>
同时设置<code class="literal">cluster.routing.allocation.node_concurrent_incoming_recoveries</code> 和 <code class="literal">cluster.routing.allocation.node_concurrent_outgoing_recoveries</code>的快捷方式。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.node_initial_primaries_recoveries</code>
</span>
</dt>
<dd>
虽然副本的恢复发生在网络上，但是节点重启后恢复未分配的主节点时使用的是本地磁盘中的数据。

这些恢复应该很快，这样就可以在同一个节点上并行地进行更多的初始主分片恢复。默认为<code class="literal">4</code>。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.same_shard.host</code>
</span>
</dt>
<dd>
允许根据主机名和主机地址执行检查，以防止在单个主机上分配同一分片的多个实例。

默认为 <code class="literal">false</code>，即不执行检查。

这个设置仅适用于在同一台机器上启动了多个节点。
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="shards-rebalancing-settings"></a>
分片再平衡设置
</h3>
</div></div></div>
<p>
以下 <em>动态的 (dynamic)</em> 设置可以用于控制跨集群的分片的再平衡：
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.rebalance.enable</code>
</span>
</dt>
<dd>
<p>
启用或禁用对特定类型的分片的再平衡：
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">all</code> -         (默认)  允许各种分片的分片再平衡
</li>
<li class="listitem">
<code class="literal">primaries</code> -   仅允许主分片的分片再平衡
</li>
<li class="listitem">
<code class="literal">replicas</code> -    仅允许副本分片的分片再平衡
</li>
<li class="listitem">
<code class="literal">none</code> -        任何索引都不允许任何类型的分片平衡。
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.allow_rebalance</code>
</span>
</dt>
<dd>
<p>
指定什么时候允许分片再平衡：
</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">always</code> -                    始终允许再平衡
</li>
<li class="listitem">
<code class="literal">indices_primaries_active</code> -  仅当集群中的所有主分片已分配(处于激活状态)
</li>
<li class="listitem">
<code class="literal">indices_all_active</code> -        (默认) 仅当集群中的所有分片(主和副本)已分配(处于激活状态)
</li>
</ul>
</div>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.cluster_concurrent_rebalance</code>
</span>
</dt>
<dd>
允许控制集群范围内允许多少个并发的分片再平衡。默认为 <code class="literal">2</code>。

请注意，此设置仅控制由于集群不平衡而导致的并发分片重定位的数量。

此设置不会限制由于 <a class="xref" href="modules-cluster.html#cluster-shard-allocation-filtering" title="Cluster-level shard allocation filtering">分配过滤 (allocation filtering)</a> 或 <a class="xref" href="modules-cluster.html#forced-awareness" title="Forced awareness">强制感知 (forced awareness)</a> 而导致的分片重新定位。
</dd>
</dl>
</div>
<div>
更多阅读参考: <a href="https://zhuanlan.zhihu.com/p/164970344" rel="nofollow">ElasticSearch集群shard均衡策略</a> (v6.x, 来自知乎)
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="shards-rebalancing-heuristics"></a>
分片平衡启发式设置 (shard balancing heuristics settings)
</h3>
</div></div></div>
<p>
下面的设置一起用于确定每个分片的放置位置。

当任何允许的再平衡操作都不能使任何节点的权重(weight) 与 任何其他节点的权重 之间的密切性比 <code class="literal">balance.threshold</code> 更接近时，集群就是平衡的<span style="color:#666; font-size:80%;">(原文: The cluster is balanced when no allowed rebalancing operation can bring the weight of any node closer to the weight of any other node by more than the balance.threshold.)</span>。
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.shard</code>
</span>
</dt>
<dd>
定义节点上分配的分片总数的权重(weight)因子(浮点型)。默认为 <code class="literal">0.45f</code>。

提高这个值会增加集群中所有节点之间分片数量均衡的趋势。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.index</code>
</span>
</dt>
<dd>
定义在特定节点上分配的每个索引的分片数的权重因子(浮点型)。默认为 <code class="literal">0.55f</code>。

提高这个值会使集群中所有节点的每个索引的分片数量更加趋于均衡。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.balance.threshold</code>
</span>
</dt>
<dd>
应该执行的最小优化值(非负浮点数)。默认为 <code class="literal">1.0f</code>。

提高该值将导致集群在优化分片平衡方面不那么积极。
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
不管平衡算法的结果如何，强制感知(forced awareness) 或 分配过滤(allocation filtering) 可能会导致不允许重新平衡。
</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="disk-based-shard-allocation"></a>
基于磁盘的分片分配设置 (Disk-based shard allocation settings)
</h3>
</div></div></div>
<p>
Elasticsearch 会考量节点上的可用磁盘空间，然后决定是向该节点分配新的分片，还是主动将分片从该节点移走。
</p>
<p>
以下是可以在配置文件 <code class="literal">elasticsearch.yml</code> 中配置的设置，或者使用 <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API 在实时集群上动态更新的设置：
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.threshold_enabled</code>
</span>
</dt>
<dd>
默认为 <code class="literal">true</code>。设置为 <code class="literal">false</code> 将禁用磁盘分配决策器。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.low</code>
</span>
</dt>
<dd>
控制磁盘使用的 低 水位线(watermark)。默认为 <code class="literal">85%</code>，表示 Elasticsearch 不会给磁盘使用率超过 85% 的节点分配分片。

也可以将其设置为绝对字节值(如<code class="literal">500mb</code>)，以防止Elasticsearch 在可用空间少于指定的值时分配分片。

该设置对新创建的索引的主分片没有影响，但会阻止它们的副本的分配。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.high</code>
</span>
</dt>
<dd>
控制磁盘使用的 高 水位线(watermark)。默认为 <code class="literal">90%</code>，表示 Elasticsearch 将尝试从磁盘使用率超过 90% 的节点上移走分片。

也可以将其设置为绝对字节值(类似于低水位线)，以便在节点的可用空间少于指定的值时将分片从该节点移走并重新定位。

这个设置影响所有分片的分配，无论之前是否已分配。
</dd>
</dl>
</div>
<div class="variablelist">
<a id="cluster-routing-flood_stage"></a>
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.watermark.flood_stage</code>
</span>
</dt>
<dd>
<p>
控制洪水位水位线，默认为 95%。

Elasticsearch 在每个索引上强制执行一个只读索引块(<code class="literal">index.blocks.read_only_allow_delete</code>)，该索引块在节点上分配了一个或多个分片，并且至少有一个磁盘超过了洪水位。

此设置是防止节点磁盘空间耗尽的最后手段。

当磁盘使用率低于高水位线时，索引块会自动释放。
</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
在这些设置中，不能混合使用百分比值和字节值。

要么所有值都设置为百分比值，要么所有值都设置为字节值。

这样做是为了让 Elasticsearch 可以验证设置在内部是一致的，确保低磁盘阈值小于高磁盘阈值，高磁盘阈值小于洪水位阈值。
</p>
</div>
</div>
<p>
重置索引<code class="literal">twitter</code>上的只读索引块的示例：
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /twitter/_settings
{
  "index.blocks.read_only_allow_delete": null
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/19.console"></div>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.info.update.interval</code>
</span>
</dt>
<dd>
Elasticsearch 应该多久检查一次集群中每个节点的磁盘使用情况。默认 <code class="literal">30s</code>。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.disk.include_relocations</code>
</span>
</dt>
<dd>
<span class="Admonishment Admonishment--change">
[<span class="Admonishment-version u-mono u-strikethrough">7.5.0</span>]
<span class="Admonishment-detail">
在 7.5.0 版本中废弃。未来的版本将始终考虑 重定位(relocations)。  
</span>
</span>
默认为 <code class="literal">true</code>，这意味着在计算节点的磁盘使用量时，Elasticsearch 会将当前重定位到目标节点的分片考虑在内。

然而，考虑重定位分片的大小，可能意味着节点的磁盘使用量被错误地估计为偏高，因为重定位可能完成了90%，而最近检索的磁盘使用量会包括重定位分片的总大小以及正在运行的重定位已经使用的空间。
</dd>
</dl>
</div>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
百分比值是指已用磁盘空间，而字节值是指可用磁盘空间。

这可能会令人困惑，因为它颠倒了高和低的含义。

例如，将低水位设置为 10gb，高水位设置为 5gb 是有意义的，但反过来则不行。
</p>
</div>
</div>
<p>
下面的例子, 将低水位线更新为至少 100gb 字节可用空间，高水位线更新为至少 50gb 字节可用空间，洪水位水位线更新为 10gb 字节可用空间，并每分钟更新一次集群的信息:
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.disk.watermark.low": "100gb",
    "cluster.routing.allocation.disk.watermark.high": "50gb",
    "cluster.routing.allocation.disk.watermark.flood_stage": "10gb",
    "cluster.info.update.interval": "1m"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/20.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="shard-allocation-awareness"></a>
分片分配感知 (shard allocation awareness)
</h3>
</div></div></div>
<p>
您可以使用自定义节点属性作为<em>感知属性(awareness attributes)</em>，使Elasticsearch 在分配分片时考虑物理硬件配置。

如果 Elasticsearch 知道哪些节点在同一个物理服务器上、在同一个机架上，或者在同一个区域中，它可以分发主分片及其副本分片，以最大限度地降低故障时丢失所有分片副本的风险。
</p>
<p>
当使用<code class="literal">cluster.routing.allocation.awareness.attributes</code>设置启用分片分配感知时，分片仅分配给具有为指定感知属性设置的值的节点。

如果使用了多个感知属性，Elasticsearch 在分配分片时会单独考虑每个属性。
</p>
<p>
分配感知设置可以在<code class="literal">elasticsearch.yml</code>中配置，也可以使用<a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API动态更新。
</p>
<p>
默认情况下，Elasticsearch 使用 <a class="xref" href="search.html#search-adaptive-replica" title="Adaptive Replica Selection">自适应副本选择(adaptive replica selection)</a> 来路由搜索或 GET 请求。

然而，由于分配感知属性的存在，Elasticsearch 将更倾向于使用相同位置的分片(具有相同的感知属性值)来处理这些请求。

要禁用此行为，可以在集群的每个节点上指定系统属性<code class="literal">export ES_JAVA_OPTS="$ES_JAVA_OPTS -Des.search.ignore_awareness_attributes=true"</code>。
</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
属性值的数量决定了在每个位置分配多少个分片的副本。

如果每个位置的节点数量不均衡，且有大量副本，则副本分片可能会保持未分配状态。
</p>
</div>
</div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="enabling-awareness"></a>
启用分片分配感知
</h4>
</div></div></div>
<p>
要启用分片分配感知:
</p>
<div class="olist orderedlist">
<ol class="orderedlist">
<li class="listitem">
<p>
使用自定义节点属性指定每个节点的位置。

例如，如果希望 Elasticsearch 将分片分布在不同的机架上，可以在每个节点的配置文件 <code class="literal">elasticsearch.yml</code> 中设置一个名为<code class="literal">rack_id</code> <span class="remark">(机架id)</span>的感知属性。
</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">node.attr.rack_id: rack_one</pre>
</div>
<p>
还可以在启动节点时设置自定义属性:
</p>
<div class="pre_wrapper lang-sh">
<pre class="programlisting prettyprint lang-sh">`./bin/elasticsearch -Enode.attr.rack_id=rack_one`</pre>
</div>
</li>
<li class="listitem">
<p>
通过在<span class="strong strong"><strong>每个</strong></span>符合主节点条件的节点的配置文件 <code class="literal">elasticsearch.yml</code> 中设置<code class="literal">cluster.routing.allocation.awareness.attributes</code>，告诉 Elasticsearch 在分配分片时考虑一个或多个感知属性。
</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">cluster.routing.allocation.awareness.attributes: rack_id <a id="CO7-1"></a><i class="conum" data-value="1"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO7-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>
以逗号分隔的列表形式指定多个属性。
</p>
</td>
</tr>
</table>
</div>
<p>
还可以使用 <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">集群更新设置</a> API来设置或更新集群的感知属性。
</p>
</li>
</ol>
</div>
<p>
在此示例配置中，如果启动两个节点且将 <code class="literal">node.attr.rack_id</code> 设置为 <code class="literal">rack_one</code> <span class="remark">(机架1)</span>，并创建一个包含 5 个主分片和每个主分片 1 个副本的索引，则所有主分片和副本都将在这两个节点之间分配。
</p>
<p>
如果在 <code class="literal">node.attr.rack_id</code> 设置为 <code class="literal">rack_two</code><span class="remark">(机架2)</span> 的情况下添加两个节点，Elasticsearch 会将分片移动到新节点，确保(如果可能的话)没有同一个分片的两个副本都在同一个机架中。
</p>
<p>
如果 <code class="literal">rack_two</code><span class="remark">(机架2)</span> 出现故障并关闭了它的两个节点，默认情况下，Elasticsearch 会将丢失的分片副本分配给 <code class="literal">rack_one</code> <span class="remark">(机架1)</span> 上的节点。

为了防止在同一位置分配特定分片的多个副本，可以启用 <em>强制感知 (forced awareness)</em>。
</p>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="forced-awareness"></a>
强制感知 (forced awareness)
</h4>
</div></div></div>
<p>
默认情况下，如果一个位置失效，Elasticsearch 会将所有丢失的副本分片分配给其余位置。

虽然可能在所有的位置上都有足够的资源来承载主分片和副本分片，但是单个位置可能无法承载<span class="strong strong"><strong>所有的</strong></span>分片。
</p>
<p>
为了防止单个位置在发生故障时超负荷，可以设置 <code class="literal">cluster.routing.allocation.awareness.force</code>，以便在其他位置的节点可用之前不会分配副本。
</p>
<p>
例如，如果有一个名为<code class="literal">zone</code>的感知属性，并在<code class="literal">zone1</code>和<code class="literal">zone2</code>中配置节点，可以使用强制感知来防止 Elasticearch 在只有一个区域可用时分配副本：
</p>
<div class="pre_wrapper lang-yaml">
<pre class="programlisting prettyprint lang-yaml">cluster.routing.allocation.awareness.attributes: zone
cluster.routing.allocation.awareness.force.zone.values: zone1,zone2 <a id="CO8-1"></a><i class="conum" data-value="1"></i></pre>
</div>
<div class="calloutlist">
<table border="0" summary="Callout list">
<tr>
<td align="left" valign="top" width="5%">
<p><a href="#CO8-1"><i class="conum" data-value="1"></i></a></p>
</td>
<td align="left" valign="top">
<p>为感知属性指定所有可能的值。</p>
</td>
</tr>
</table>
</div>
<p>
使用上面这个示例的配置，如果启动两个节点，并将<code class="literal">node.attr.zone</code> 设置为 <code class="literal">zone1</code>，并创建一个包含 5 个分片和 1 个副本的索引，则 Elasticsearch 将创建索引并分配 5 个主分片，但不分配副本。副本只有在 <code class="literal">node.attr.zone</code> 值为<code class="literal">zone2</code> 的节点可用时才会分配。
</p>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="cluster-shard-allocation-filtering"></a>
集群级分片分配过滤 (cluster-level shard allocation filtering)
</h3>
</div></div></div>
<p>
可以使用群集级别的分片分配过滤器来控制 Elasticearch 从任何索引中分配分片的位置。

这些集群范围的过滤器与 <a class="xref" href="shard-allocation-filtering.html" title="Index-level shard allocation filtering">每个索引的分配过滤</a> 和 <a class="xref" href="modules-cluster.html#shard-allocation-awareness" title="Shard allocation awareness">分配感知</a> 一起应用。
</p>
<p>
分片分配过滤可以基于自定义的节点属性，或者内置的<code class="literal">_name</code>, <code class="literal">_host_ip</code>, <code class="literal">_publish_ip</code>, <code class="literal">_ip</code>, <code class="literal">_host</code> 和 <code class="literal">_id</code>属性。
</p>
<p>
<code class="literal">cluster.routing.allocation</code> 是动态的，允许将活动索引从一组节点移动到另一组节点。

只有在不破坏另一个路由约束的情况下，才能重新定位分片，例如永远不要在同一节点上分配主分片和副本分片。
</p>
<p>
集群级分片分配过滤最常见的使用场景是当你想 关闭<span class="remark">(decommission)</span> 一个节点时。

若要在关闭节点之前将分片移出节点，可以创建一个过滤器，根据节点的 IP 地址将其排除在外：
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "transient" : {
    "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/21.console"></div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-routing-settings"></a>
集群路由设置
</h4>
</div></div></div>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.include.{attribute}</code>
</span>
</dt>
<dd>
将分片分配给 <code class="literal">{attribute}</code> 至少有一个逗号分隔值的节点。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.require.{attribute}</code>
</span>
</dt>
<dd>
仅将分片分配给 <code class="literal">{attribute}</code> 具有<em>所有</em>逗号分隔值的节点。
</dd>
<dt>
<span class="term">
<code class="literal">cluster.routing.allocation.exclude.{attribute}</code>
</span>
</dt>
<dd>
不要将分片分配给其<code class="literal">{attribute}</code>具有<em>任何</em>逗号分隔值的节点。
</dd>
</dl>
</div>
<p>
集群分配设置支持以下几个内置的属性：
</p>
<div class="informaltable">
<table border="0" cellpadding="4px">
<colgroup>
<col>
<col>
</colgroup>
<tbody valign="top">
<tr>
<td valign="top">
<p>
<code class="literal">_name</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by node name
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_host_ip</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by host IP address (IP associated with hostname)
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_publish_ip</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by publish IP address
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_ip</code>
</p>
</td>
<td valign="top">
<p>
Match either <code class="literal">_host_ip</code> or <code class="literal">_publish_ip</code>
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_host</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by hostname
</p>
</td>
</tr>
<tr>
<td valign="top">
<p>
<code class="literal">_id</code>
</p>
</td>
<td valign="top">
<p>
Match nodes by node id
</p>
</td>
</tr>
</tbody>
</table>
</div>
<p>You can use wildcards when specifying attribute values, for example:</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT _cluster/settings
{
  "transient": {
    "cluster.routing.allocation.exclude._ip": "192.168.2.*"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/22.console"></div>
</div>

</div>

<div class="section">
<div class="titlepage"><div><div>
<h3 class="title">
<a id="misc-cluster-settings"></a>
Miscellaneous cluster settings
</h3>
</div></div></div>
<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-read-only"></a>Metadata
</h4>
</div></div></div>
<p>An entire cluster may be set to read-only with the following <em>dynamic</em> setting:</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.blocks.read_only</code>
</span>
</dt>
<dd>
Make the whole cluster read only (indices do not accept write operations), metadata is not allowed to be modified (create or delete indices).
</dd>
<dt>
<span class="term">
<code class="literal">cluster.blocks.read_only_allow_delete</code>
</span>
</dt>
<dd>
Identical to <code class="literal">cluster.blocks.read_only</code> but allows to delete indices to free up resources.
</dd>
</dl>
</div>
<div class="warning admon">
<div class="icon"></div>
<div class="admon_content">
<p>Don’t rely on this setting to prevent changes to your cluster. Any user with access to the <a class="xref" href="cluster-update-settings.html" title="Cluster update settings API">cluster-update-settings</a> API can make the cluster read-write again.</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-shard-limit"></a>
Cluster shard limit
</h4>
</div></div></div>
<p>
There is a soft limit on the number of shards in a cluster, based on the number of nodes in the cluster.

This is intended to prevent operations which may unintentionally destabilize the cluster.
</p>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>
This limit is intended as a safety net, not a sizing recommendation.

The exact number of shards your cluster can safely support depends on your hardware configuration and workload, but should remain well below this limit in almost all cases, as the default limit is set quite high.
</p>
</div>
</div>
<p>
If an operation, such as creating a new index, restoring a snapshot of an index, or opening a closed index would lead to the number of shards in the cluster going over this limit, the operation will fail with an error indicating the shard limit.
</p>
<p>
If the cluster is already over the limit, due to changes in node membership or setting changes, all operations that create or open indices will fail until either the limit is increased as described below, or some indices are <a class="xref" href="indices-open-close.html" title="Open index API">closed</a> or <a class="xref" href="indices-delete-index.html" title="Delete index API">deleted</a> to bring the number of shards below the limit.
</p>
<p>
Replicas count towards this limit, but closed indexes do not.

An index with 5 primary shards and 2 replicas will be counted as 15 shards.

Any closed index is counted as 0, no matter how many shards and replicas it contains.
</p>
<p>
The limit defaults to 1,000 shards per data node, and can be dynamically adjusted using the following property:
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.max_shards_per_node</code>
</span>
</dt>
<dd>
Controls the number of shards allowed in the cluster per data node.
</dd>
</dl>
</div>
<p>
For example, a 3-node cluster with the default setting would allow 3,000 shards total, across all open indexes.

If the above setting is changed to 500, then the cluster would allow 1,500 shards total.
</p>
<div class="note admon">
<div class="icon"></div>
<div class="admon_content">
<p>
If there are no data nodes in the cluster, the limit will not be enforced.

This allows the creation of indices during cluster creation if dedicated master nodes are set up before data nodes.
</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="user-defined-data"></a>
User-defined cluster metadata
</h4>
</div></div></div>
<p>
User-defined metadata can be stored and retrieved using the Cluster Settings API.

This can be used to store arbitrary, infrequently-changing data about the cluster without the need to create an index to store it.

This data may be stored using any key prefixed with <code class="literal">cluster.metadata.</code>.

For example, to store the email address of the administrator of a cluster under the key <code class="literal">cluster.metadata.administrator</code>, issue this request:
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /_cluster/settings
{
  "persistent": {
    "cluster.metadata.administrator": "sysadmin@example.com"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/23.console"></div>
<div class="important admon">
<div class="icon"></div>
<div class="admon_content">
<p>
User-defined cluster metadata is not intended to store sensitive or
confidential information.

Any information stored in user-defined cluster metadata will be viewable by anyone with access to the <a class="xref" href="cluster-get-settings.html" title="Cluster get settings API">Cluster Get Settings</a> API, and is recorded in the Elasticsearch logs.
</p>
</div>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-max-tombstones"></a>
Index tombstones
</h4>
</div></div></div>
<p>
The cluster state maintains index tombstones to explicitly denote indices that have been deleted.

The number of tombstones maintained in the cluster state is controlled by the following property, which cannot be updated dynamically:
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.indices.tombstones.size</code>
</span>
</dt>
<dd>
Index tombstones prevent nodes that are not part of the cluster when a delete occurs from joining the cluster and reimporting the index as though the delete was never issued.

To keep the cluster state from growing huge we only keep the last <code class="literal">cluster.indices.tombstones.size</code> deletes, which defaults to 500.

You can increase it if you expect nodes to be absent from the cluster and miss more than 500 deletes.

We think that is rare, thus the default.

Tombstones don’t take up much space, but we also think that a number like 50,000 is probably too big.
</dd>
</dl>
</div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="cluster-logger"></a>
Logger
</h4>
</div></div></div>
<p>
The settings which control logging can be updated dynamically with the <code class="literal">logger.</code> prefix.

For instance, to increase the logging level of the <code class="literal">indices.recovery</code> module to <code class="literal">DEBUG</code>, issue this request:
</p>
<div class="pre_wrapper lang-console">
<pre class="programlisting prettyprint lang-console">PUT /_cluster/settings
{
  "transient": {
    "logger.org.elasticsearch.indices.recovery": "DEBUG"
  }
}</pre>
</div>
<div class="console_widget" data-snippet="snippets/24.console"></div>
</div>

<div class="section">
<div class="titlepage"><div><div>
<h4 class="title">
<a id="persistent-tasks-allocation"></a>
Persistent tasks allocation
</h4>
</div></div></div>
<p>
Plugins can create a kind of tasks called persistent tasks.

Those tasks are usually long-lived tasks and are stored in the cluster state, allowing the tasks to be revived after a full cluster restart.
</p>
<p>
Every time a persistent task is created, the master node takes care of assigning the task to a node of the cluster, and the assigned node will then pick up the task and execute it locally.

The process of assigning persistent tasks to nodes is controlled by the following properties, which can be updated dynamically:
</p>
<div class="variablelist">
<dl class="variablelist">
<dt>
<span class="term">
<code class="literal">cluster.persistent_tasks.allocation.enable</code>
</span>
</dt>
<dd>
<p>Enable or disable allocation for persistent tasks:</p>
<div class="ulist itemizedlist">
<ul class="itemizedlist">
<li class="listitem">
<code class="literal">all</code> -             (default) Allows persistent tasks to be assigned to nodes
</li>
<li class="listitem">
<code class="literal">none</code> -            No allocations are allowed for any type of persistent task
</li>
</ul>
</div>
<p>
This setting does not affect the persistent tasks that are already being executed.

Only newly created persistent tasks, or tasks that must be reassigned (after a node left the cluster, for example), are impacted by this setting.
</p>
</dd>
<dt>
<span class="term">
<code class="literal">cluster.persistent_tasks.allocation.recheck_interval</code>
</span>
</dt>
<dd>
The master node will automatically check whether persistent tasks need to be assigned when the cluster state changes significantly.

However, there may be other factors, such as memory usage, that affect whether persistent tasks can be assigned to nodes but do not cause the cluster state to change.

This setting controls how often assignment checks are performed to react to these factors.

The default is 30 seconds.

The minimum permitted value is 10 seconds.
</dd>
</dl>
</div>
</div>

</div>

</div>
<div class="navfooter">
<span class="prev">
<a href="circuit-breaker.html">« 熔断机制设置</a>
</span>
<span class="next">
<a href2="ccr-settings.html">Cross-cluster replication settings »</a>
</span>
</div>
</div>

                  <!-- end body -->
                        </div>
                        <div class="col-xs-12 col-sm-4 col-md-4" id="right_col">
                        
                        </div>
                    </div>
                </div>
            </section>
        </div>
    </section>
</div>
<script src="../static/cn.js"></script>
</body>
</html>